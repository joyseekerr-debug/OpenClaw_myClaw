# Phase 3-12: Transformer架构

## 一、概念定义

### 1.1 Transformer模型
一种基于自注意力机制（Self-Attention）的深度学习架构，彻底改变了自然语言处理领域，并逐渐应用于时间序列预测等任务。

**核心突破（2017年Google "Attention Is All You Need"）：**
- **摒弃循环结构**：完全基于注意力，可并行计算
- **长距离依赖**：直接建模任意距离的关系
- **位置编码**：显式建模序列位置信息
- **可扩展性**：支持大规模预训练

### 1.2 注意力机制（Attention Mechanism）
一种动态加权机制，让模型在处理序列时自动关注输入的不同部分。

**核心思想：**
```
"在处理当前元素时，我应该关注输入序列的哪些部分？"

类比：
• 阅读理解：读到一个词时，回顾前文相关词
• 翻译：生成目标词时，关注源句子相关词
• 时序预测：预测未来时，关注历史关键时期
```

**注意力权重：**
```
对于每个查询（Query），计算与所有键（Key）的相似度
相似度经过softmax得到注意力权重
用权重对值（Value）加权求和得到输出
```

### 1.3 Transformer vs RNN/LSTM

| 特性 | RNN/LSTM | Transformer |
|------|----------|-------------|
| 计算方式 | 串行 | 并行 |
| 长距离依赖 | 逐步传递，易丢失 | 直接连接，全局建模 |
| 位置信息 | 隐式（按顺序处理） | 显式（位置编码） |
| 训练速度 | 慢 | 快 |
| 内存需求 | 低 | 高（随序列长度平方增长） |
| 适合场景 | 短序列、实时推理 | 长序列、批处理训练 |

---

## 二、算法原理

### 2.1 自注意力机制（Self-Attention）

**Scaled Dot-Product Attention**
```
计算公式：
Attention(Q, K, V) = softmax(QK^T / √d_k) × V

其中：
• Q（Query）：查询矩阵 [seq_len, d_k]
• K（Key）：键矩阵 [seq_len, d_k]
• V（Value）：值矩阵 [seq_len, d_v]
• d_k：键向量的维度
• √d_k：缩放因子，防止softmax梯度消失

计算步骤：
1. Q × K^T：计算查询与所有键的相似度 [seq_len, seq_len]
2. / √d_k：缩放
3. softmax：归一化为概率分布
4. × V：加权求和得到输出
```

**多头注意力（Multi-Head Attention）**
```
核心思想：不同子空间学习不同类型的关系

计算过程：
1. 将Q、K、V线性投影h次到不同子空间
2. 在每个子空间并行计算注意力
3. 拼接所有头的输出
4. 线性变换得到最终输出

公式：
MultiHead(Q, K, V) = Concat(head_1, ..., head_h) × W^O
where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)

优势：
• 不同头学习不同模式（趋势、周期、异常点）
• 增强表达能力
• 标准配置：8头或16头
```

### 2.2 Transformer编码器结构

**完整编码器层：**
```
┌─────────────────────────────────────────┐
│           Transformer编码器层            │
├─────────────────────────────────────────┤
│  输入：x（带位置编码的嵌入向量）           │
│                                         │
│  ┌─────────────────────────────────┐    │
│  │  多头自注意力子层                │    │
│  │  MultiHeadAttention(x, x, x)    │    │
│  │  ↓                             │    │
│  │  Add & Norm（残差连接+层归一化）│    │
│  └─────────────────────────────────┘    │
│              ↓                          │
│  ┌─────────────────────────────────┐    │
│  │  前馈神经网络子层                │    │
│  │  FFN(x) = ReLU(xW1 + b1)W2 + b2 │    │
│  │  ↓                             │    │
│  │  Add & Norm                    │    │
│  └─────────────────────────────────┘    │
│                                         │
│  输出：编码后的表示                      │
└─────────────────────────────────────────┘

关键组件：
• 残差连接（Residual Connection）：缓解梯度消失
• 层归一化（Layer Normalization）：稳定训练
• FFN：每个位置独立应用，增加非线性
```

### 2.3 位置编码（Positional Encoding）

**目的**
```
Transformer无循环、无卷积，需要显式告知位置信息
```

**正弦位置编码（原版）**
```
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

其中：
• pos：位置
• i：维度索引
• d_model：模型维度

特点：
• 每个位置有唯一编码
• 可学习相对位置（PE(pos+k)是PE(pos)的线性函数）
• 可外推到更长序列
```

**可学习位置编码**
```
直接将位置编码作为可训练参数
优点：灵活适应数据
缺点：无法外推到训练时未见过的长度
```

**相对位置编码**
```
不编码绝对位置，而是编码位置间相对关系
适合时序预测：更关注时间间隔而非绝对时间
```

### 2.4 解码器结构（Decoder）

**结构差异**
```
解码器比编码器多一个子层：
1. 带掩码的多头自注意力（防止看到未来）
2. 编码器-解码器注意力（关注编码器输出）
3. 前馈神经网络

掩码机制：
• 在注意力计算中屏蔽未来位置
• 保证自回归特性
• 适合序列生成任务
```

### 2.5 时序适配变体

**Informer（长序列时序预测）**
```
改进点：
• ProbSparse Attention：只关注重要查询
• 自注意力蒸馏：下采样减少复杂度
• 生成式解码器：直接预测长序列

复杂度：从O(L²)降到O(L log L)
```

**Autoformer（自相关机制）**
```
改进点：
• 自相关机制：基于周期性的时延聚合
• 分解架构：趋势+季节性的显式建模
• 更适合周期性强的时序数据
```

**PatchTST**
```
改进点：
• 将时间序列分块（patch）
• 使用通道独立（channel independence）
• 在长序列预测上表现优异
```

---

## 三、应用场景

### 3.1 金融时序预测

**直接预测**
```
输入：历史价格序列窗口
输出：未来价格或收益率

模型配置：
• 编码器-only结构
• 输出层映射到预测值
• 回归损失（MSE/MAE）

优势：
• 直接建模长程依赖
• 并行训练效率高
• 可处理多变量序列
```

**分解预测**
```
趋势-季节性分解：
• 趋势组件：长期方向（低维）
• 季节组件：周期性波动
• 残差组件：随机噪声

实现方式：
• 前处理分解（STL等）
• 模型内部分解（Autoformer）
• 多分支网络分别建模
```

### 3.2 多变量关系建模

**资产关系学习**
```
输入：多资产收益率序列
• 股票池内所有股票
• 跨资产类别（股、债、商）

学习到的关系：
• 相关性动态变化
• 领先滞后关系
• 风险传导路径

应用：
• 组合风险预测
• 配对交易机会识别
• 资产配置优化
```

**跨市场分析**
```
全球市场关联：
• 美股、欧股、亚股的相互影响
• 外汇市场联动
• 大宗商品与股市关系

注意力权重解释：
• 高权重 = 强关联
• 动态变化 = 关系演变
```

### 3.3 事件驱动策略

**新闻情感分析**
```
文本输入 → Transformer编码 → 情感得分

预训练模型：
• FinBERT（金融领域BERT）
• 在财经新闻上微调

应用：
• 实时新闻情感打分
• 事件冲击预测
• 舆情因子构建
```

**财报分析**
```
输入：财报文本
任务：
• 关键信息提取
• 情感分类
• 超预期/低于预期判断

结合时序：
• 历史财报趋势
• 与股价反应关联
```

### 3.4 与传统模型结合

**特征提取器**
```
Transformer → 时序特征 → XGBoost/线性模型

流程：
1. Transformer学习时序表示
2. 取最后一层或CLS token
3. 与传统因子拼接
4. 输入树模型或线性模型
```

**集成策略**
```
多模型集成：
• Transformer：长程依赖
• LSTM：局部模式
• XGBoost：横截面关系

融合方式：
• 预测值加权平均
• 堆叠（Stacking）
• 门控融合网络
```

---

## 四、注意事项

### 4.1 计算复杂度

**自注意力的O(n²)问题**
```
问题：
• 序列长度n，注意力计算复杂度O(n²)
• 内存需求随长度平方增长
• 长序列（如日线多年数据）难以处理

限制示例：
• 512长度：可行
• 4096长度：需要较大内存
• 10000+长度：需特殊优化

解决方案：
• 序列分块（Patching）
• 稀疏注意力（Sparse Attention）
• 线性注意力（Linear Attention）
• 使用Informer等高效变体
```

**金融数据限制**
```
问题：
• 金融序列长度有限
• 多年日线数据可能仅几千点
• 与NLP任务（百万token）不同

适配策略：
• 合适的序列窗口（60-252日）
• 多资产联合建模增加数据
• 使用数据增强
```

### 4.2 过拟合风险

**参数规模**
```
Transformer参数量大：
• 容易过拟合小样本金融数据
• 需要强正则化
• 或使用预训练+微调

正则化策略：
• Dropout（注意力和FFN）
• 权重衰减
• 早停
• 标签平滑
```

**数据增强**
```
时序增强方法：
• 随机缩放（幅度扰动）
• 随机偏移
• 时间 warping
• 混合（Mixup）

注意事项：
• 保持时序特性
• 不改变标签含义
• 模拟真实市场变化
```

### 4.3 可解释性挑战

**黑盒问题**
```
挑战：
• 注意力权重分布复杂
• 难以解释预测依据
• 监管和风控需要

部分解决方案：
• 注意力可视化
• 分析哪些历史点被重点关注
• 与已知市场事件对比
```

**注意力分析**
```
可视化方法：
• 热力图显示注意力权重
• 识别关注的关键时间步
• 多头分别分析

金融洞察：
• 是否关注重要事件日期
• 注意力是否集中在近期
• 不同市场状态下的注意力模式
```

### 4.4 时序安全

**因果约束**
```
❌ 使用双向注意力预测未来
❌ 位置编码泄露未来信息
❌ 全局统计量标准化

✅ 预测任务使用因果掩码
✅ 只用历史信息计算位置编码
✅ 滚动窗口标准化
```

**未来信息泄露检查**
```
验证清单：
□ 注意力是否只关注过去
□ 标准化是否只用历史数据
□ 特征构造是否有未来函数
□ 标签定义是否正确
```

---

## 五、核心概念总结

```
┌─────────────────────────────────────────────────────────┐
│               Transformer核心框架                        │
├─────────────────────────────────────────────────────────┤
│  核心创新                                              │
│  ├── 自注意力：直接建模任意位置间关系                   │
│  ├── 并行计算：摒弃循环，高效训练                       │
│  ├── 位置编码：显式引入序列顺序信息                     │
│  └── 可扩展性：支持大规模预训练                         │
├─────────────────────────────────────────────────────────┤
│  关键组件                                              │
│  ├── 多头注意力：多子空间并行学习不同模式               │
│  ├── 前馈网络：位置独立非线性变换                       │
│  ├── 残差连接：缓解梯度消失，支持深层                   │
│  ├── 层归一化：稳定训练动态                             │
│  └── 位置编码：注入位置信息                             │
├─────────────────────────────────────────────────────────┤
│  金融应用                                              │
│  ├── 时序预测：直接建模长程依赖                         │
│  ├── 关系学习：多资产动态相关性                         │
│  ├── 文本分析：新闻情感、财报解读                       │
│  └── 混合架构：与时序/树模型结合                        │
├─────────────────────────────────────────────────────────┤
│  关键注意                                              │
│  ├── 复杂度限制：O(n²)内存需求，长序列需优化            │
│  ├── 过拟合：参数量大，需强正则化                       │
│  ├── 可解释性：注意力可视化辅助理解                     │
│  ├── 因果约束：预测任务需确保只用过去信息               │
│  └── 数据适配：金融序列短，需合适窗口设计               │
└─────────────────────────────────────────────────────────┘
```

---

*文档创建时间：2026-02-24*  
*所属阶段：Phase 3 - 量化技能（第12/20项）*
