# Phase 3-3: 数据清洗与预处理

## 一、概念定义

### 1.1 数据清洗（Data Cleaning）
识别并纠正数据中的错误、不一致、重复和缺失值，以提高数据质量的过程。

**主要目标：**
- **完整性**：处理缺失值、填补数据缺口
- **准确性**：识别并修正错误数据
- **一致性**：统一格式、消除矛盾
- **有效性**：移除重复和异常值

### 1.2 数据预处理（Data Preprocessing）
将原始数据转换为适合机器学习或量化分析的格式的过程。

**核心步骤：**
- **标准化/归一化**：统一数值范围
- **编码转换**：类别变量数值化
- **特征构造**：从原始数据创建新特征
- **降维**：减少特征数量

### 1.3 金融数据特殊问题

| 问题类型 | 描述 | 示例 |
|----------|------|------|
| 幸存者偏差 | 只包含现存公司数据 | 回测时忽略已退市股票 |
| 前视偏差 | 使用未来信息 | 财报发布前使用未公布数据 |
| 价格跳跃 | 除权除息导致的跳空 | 股票分红后价格突然下降 |
| 停牌处理 | 交易中断期间的数据 | 缺失交易日如何处理 |
| 时间对齐 | 不同市场时区差异 | A股与美股数据合并 |

---

## 二、算法原理

### 2.1 缺失值处理算法

**识别缺失模式**
```
MCAR（完全随机缺失）：缺失与任何值无关
MAR（随机缺失）：缺失与其他观测值有关
MNAR（非随机缺失）：缺失与缺失值本身有关

检测方法：
• Little's MCAR检验
• 可视化缺失模式热力图
• 按时间/类别分析缺失分布
```

**缺失值填充策略**

| 方法 | 适用场景 | 原理 |
|------|----------|------|
| 删除法 | 缺失率低（<5%） | 直接删除缺失行/列 |
| 均值/中位数填充 | 数值型、分布对称 | 用统计量替代 |
| 前向/后向填充 | 时间序列 | ffill/bfill |
| 线性插值 | 连续变化数据 | 相邻点线性估计 |
| KNN填充 | 有相似样本 | 最近邻加权平均 |
| 模型预测填充 | 复杂关系 | 用其他特征预测 |

**金融数据特殊处理**
```
停牌处理：
• 标记而非填充：保留停牌信息
• 前向填充需谨慎：避免传播过时价格
• 收益率计算：停牌期间设为NaN

除权除息处理：
• 复权价格计算
• 前复权：以当前为基准调整历史
• 后复权：以上市日为基准
```

### 2.2 异常值检测算法

**统计方法**
```
Z-Score方法：
• 阈值通常为 |z| > 3
• 假设数据服从正态分布
• 公式：z = (x - μ) / σ

IQR方法（稳健）：
• Q1 - 1.5×IQR 到 Q3 + 1.5×IQR 为正常范围
• 不受极端值影响
• 适合非对称分布

Modified Z-Score：
• 使用中位数替代均值
• 使用MAD（绝对中位差）替代标准差
```

**机器学习方法**
```
孤立森林（Isolation Forest）：
• 基于随机划分的异常检测
• 异常点更容易被孤立
• 时间复杂度低，适合大数据

LOF（局部异常因子）：
• 比较局部密度差异
• 能发现簇内异常
• 计算复杂度较高

单类SVM：
• 学习正常数据边界
• 边界外为异常
• 适合高维数据
```

### 2.3 数据标准化与变换

**标准化方法**

| 方法 | 公式 | 适用场景 |
|------|------|----------|
| Min-Max缩放 | (x - min) / (max - min) | 需要固定范围[0,1] |
| Z-Score标准化 | (x - μ) / σ | 数据近似正态分布 |
| Robust标准化 | (x - median) / IQR | 有异常值的情况 |
| 对数变换 | log(x) | 右偏分布、大数值范围 |
| Box-Cox变换 | (x^λ - 1) / λ | 寻找最优幂变换 |
| Yeo-Johnson | 扩展Box-Cox支持负值 | 含负值的数据 |

**金融数据标准化特殊考虑**
```
收益率标准化：
• 避免在波动率聚集期失真
• 考虑使用滚动窗口标准化
• 注意胖尾分布特性

量价数据：
• 价格与成交量分别处理
• 考虑对数收益率而非价格
• 行业/市值中性化
```

### 2.4 时间序列特殊处理

**时间对齐**
```
交易日历统一：
• 处理不同市场节假日差异
• 统一时间戳格式（UTC）
• 对齐开盘/收盘时间

频率转换：
• 升采样（Upsampling）：插值方法选择
• 降采样（Downsampling）：聚合函数选择
• 注意标签位置（左/右闭合）
```

**序列平稳性处理**
```
差分：
• 一阶差分：消除趋势
• 季节性差分：消除季节效应
• 对数差分：处理异方差性

去趋势：
• 移动平均去趋势
• 多项式拟合去趋势
• HP滤波（Hodrick-Prescott）
```

---

## 三、应用场景

### 3.1 量化因子数据准备

**典型处理流程**
```
原始数据 → 清洗 → 标准化 → 中性化 → 因子值

1. 极值处理（Winsorize）：限制在±3σ或1%/99%分位
2. 缺失值填充：行业均值或中位数
3. 标准化：Z-Score或Rank标准化
4. 中性化：去除行业和市值影响
5. 最终因子值
```

**行业中性化**
```
目的：消除行业差异，使因子可比

方法：
• 回归中性化：对行业虚拟变量回归，取残差
• 分层标准化：行业内独立标准化
• 市值中性化：同时控制市值影响
```

### 3.2 机器学习数据准备

**训练/验证/测试划分**
```
时间序列划分原则：
• 严禁随机划分（导致前视偏差）
• 按时间顺序：训练 → 验证 → 测试
• 滚动窗口验证（Walk-forward）

典型比例：
• 训练集：60-70%
• 验证集：15-20%
• 测试集：15-20%
```

**特征工程流水线**
```
原始特征 → 清洗 → 变换 → 选择 → 模型输入

清洗：处理缺失、异常
变换：标准化、编码、降维
选择：相关性分析、重要性排序
```

### 3.3 回测数据准备

**数据质量检查清单**
```
□  survivor bias检查：包含退市股票
□  look-ahead bias检查：数据时点验证
□  价格连续性：无跳空（除权除息已处理）
□  成交量合理性：非零且不过于异常
□  时间对齐：所有股票时间戳一致
□  复权因子：除权除息已正确调整
```

---

## 四、注意事项

### 4.1 常见陷阱

**前视偏差（Look-ahead Bias）**
```
❌ 使用当天收盘价计算信号，当天交易
❌ 用月末财务数据在月末前交易
❌ 使用未来信息填充历史缺失值

✅ 使用上一交易日收盘价
✅ 财报发布后的第一个交易日再使用
✅ 只用当时可获取的信息
```

**幸存者偏差（Survivorship Bias）**
```
❌ 只用当前成分股历史数据回测
❌ 忽略已退市、被合并的公司

✅ 获取完整历史成分股列表
✅ 包含退市股票的历史数据
✅ 记录成分股变更事件
```

**数据窥探（Data Snooping）**
```
❌ 反复优化参数直到测试集表现好
❌ 在测试集上调整策略逻辑
❌ 使用相同数据多次验证

✅ 严格区分训练/验证/测试集
✅ 使用样本外数据验证
✅ 记录所有实验尝试
```

### 4.2 清洗过度风险

| 过度清洗表现 | 后果 | 建议 |
|--------------|------|------|
| 删除过多异常值 | 丢失真实信号 | 使用稳健统计量替代删除 |
| 过度平滑 | 消除波动特征 | 保留原始数据副本 |
| 强制填充所有缺失 | 引入虚假模式 | 允许合理缺失 |
| 过度标准化 | 破坏原始关系 | 记录变换参数 |

### 4.3 文档与可追溯性

**必要记录**
```
数据血缘（Data Lineage）：
• 原始数据来源
• 所有变换步骤
• 参数设置
• 执行时间戳

版本控制：
• 清洗代码版本
• 数据处理配置
• 输入数据版本
• 输出数据版本
```

**可重复性原则**
```
✅ 代码化所有清洗步骤（避免手动操作）
✅ 使用确定性算法（设置随机种子）
✅ 保存中间结果便于审计
✅ 单元测试关键清洗逻辑
```

### 4.4 性能优化

**大数据处理策略**
```
分块处理：
• 按股票分块处理
• 按时间窗口分批
• 使用生成器避免内存溢出

并行计算：
• 多进程处理独立股票
• 注意GIL限制（Python）
• 考虑使用Dask/Spark

存储优化：
• 使用Parquet替代CSV
• 选择合适的数据类型
• 压缩存储历史数据
```

---

## 五、核心概念总结

```
┌─────────────────────────────────────────────────────────┐
│                 数据清洗与预处理流程                     │
├─────────────────────────────────────────────────────────┤
│  1. 探索性分析（EDA）                                    │
│     ├── 统计摘要：均值、方差、分位数                     │
│     ├── 分布可视化：直方图、箱线图                       │
│     └── 缺失分析：缺失率、缺失模式                       │
├─────────────────────────────────────────────────────────┤
│  2. 数据清洗                                             │
│     ├── 缺失值处理：识别 → 分析原因 → 选择策略 → 填充   │
│     ├── 异常值处理：检测 → 验证 → 处理（删除/修正）     │
│     ├── 重复值处理：识别 → 保留/删除                     │
│     └── 格式标准化：类型转换、单位统一                   │
├─────────────────────────────────────────────────────────┤
│  3. 数据变换                                             │
│     ├── 标准化/归一化                                   │
│     ├── 编码转换（类别变量）                            │
│     ├── 时间序列处理（差分、去趋势）                    │
│     └── 特征构造                                        │
├─────────────────────────────────────────────────────────┤
│  4. 质量验证                                             │
│     ├── 清洗前后对比                                    │
│     ├── 业务规则校验                                    │
│     └── 统计检验                                        │
└─────────────────────────────────────────────────────────┘

关键原则：清洗过程必须可重复、可审计、文档化
```

---

*文档创建时间：2026-02-24*  
*所属阶段：Phase 3 - 量化技能（第3/20项）*
