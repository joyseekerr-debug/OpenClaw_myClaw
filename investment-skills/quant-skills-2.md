# 投资技能文档：量化技能深化（二）

技能编号：162-166 / 200+

---

## 162. 神经网络基础

### 概念定义
神经网络是一种模拟生物神经系统的计算模型，由相互连接的神经元（节点）组成，能够学习复杂的非线性映射关系。

### 基本结构
- **输入层**：接收特征数据
- **隐藏层**：进行非线性变换，可有多层
- **输出层**：产生预测结果
- **连接权重**：通过学习调整的参数

### 核心概念
1. **前向传播**：数据从输入到输出的计算过程
2. **激活函数**：引入非线性（ReLU、Sigmoid、Tanh）
3. **反向传播**：根据误差调整权重的学习算法
4. **损失函数**：衡量预测与真实值的差异

### 网络类型
- **前馈神经网络（FNN）**：最简单的全连接网络
- **卷积神经网络（CNN）**：擅长处理图像和时序局部特征
- **循环神经网络（RNN）**：处理序列数据，有记忆能力
- **长短时记忆网络（LSTM）**：解决RNN长期依赖问题

### 金融应用
- **价格预测**：基于历史价格序列预测未来走势
- **模式识别**：识别K线形态和图表模式
- **高频交易**：微秒级信号生成
- **风险建模**：非线性风险因子分析

### 注意事项
- 金融数据信噪比低，需防范过拟合
- 网络深度与数据量需匹配
- 训练需要大量计算资源
- 黑盒特性导致可解释性差
- 市场状态变化需定期重训练

---

## 163. GAN生成对抗网络

### 概念定义
生成对抗网络由生成器和判别器两部分组成，通过对抗训练的方式学习数据分布，能够生成逼真的合成数据。

### 原理
- **生成器（G）**：学习生成与真实数据相似的样本
- **判别器（D）**：学习区分真实数据与生成数据
- **对抗博弈**：G试图欺骗D，D试图识别真伪，达到纳什均衡

### 损失函数
```
min_G max_D V(D,G) = E[log D(x)] + E[log(1-D(G(z)))]
```

### 金融应用场景
- **数据增强**：生成更多训练样本，解决数据不足
- **情景模拟**：生成极端市场情景用于压力测试
- **尾部风险**：生成稀有事件样本完善风险模型
- **市场模拟**：生成合成价格路径用于回测
- **隐私保护**：用合成数据替代敏感真实数据

### GAN变体
- **CGAN**：条件GAN，可控制生成样本的类别
- **WGAN**：使用Wasserstein距离，训练更稳定
- **DCGAN**：使用卷积层，适合图像类数据
- **TimeGAN**：专门用于时间序列数据生成

### 注意事项
- 训练不稳定，容易模式崩溃
- 生成数据质量评估困难
- 金融数据的时序特性需特殊处理
- 生成数据可能与真实数据分布有偏差
- 需谨慎使用合成数据做实盘决策

---

## 164. 强化学习基础

### 概念定义
强化学习是智能体通过与环境交互，根据奖励反馈学习最优行为策略的机器学习方法。

### 核心要素
- **智能体（Agent）**：执行动作的决策者
- **环境（Environment）**：智能体所处的世界
- **状态（State）**：环境的当前状况
- **动作（Action）**：智能体可执行的操作
- **奖励（Reward）**：环境的反馈信号
- **策略（Policy）**：状态到动作的映射

### 关键概念
1. **探索与利用**：尝试新动作 vs 使用已知最优动作
2. **折扣因子**：未来奖励的折现率
3. **价值函数**：评估状态的长期回报期望
4. **策略梯度**：直接优化策略参数

### 金融应用
- **算法交易**：学习最优下单时机和数量
- **组合管理**：动态调整资产配置
- **做市策略**：最优报价策略学习
- **风险管理**：学习最优对冲策略
- **期权定价**：美式期权最优行权策略

### 主要算法
- **价值迭代**：Q-Learning、SARSA
- **策略梯度**：REINFORCE、Actor-Critic
- **模型基础**：蒙特卡洛树搜索（MCTS）

### 注意事项
- 市场非平稳性导致训练困难
- 奖励函数设计影响学习效果
- 需要大量交互数据
- 实盘风险高，需充分模拟测试
- 探索过程可能导致较大亏损

---

## 165. Q-Learning

### 概念定义
Q-Learning是一种无模型的强化学习算法，通过学习动作价值函数Q(s,a)来找到最优策略。

### 核心公式
```
Q(s,a) ← Q(s,a) + α[r + γ·max(Q(s',a')) - Q(s,a)]
```
- **α**：学习率
- **γ**：折扣因子
- **max(Q(s',a'))**：下一状态的最大Q值

### 算法特点
- **离线学习**：可以从任意经验中学习，不一定遵循当前策略
- **时序差分**：结合当前估计和实际奖励更新
- **收敛性**：在满足条件下可收敛到最优Q函数

### 探索策略
1. **ε-贪心**：以ε概率随机探索，1-ε概率选择最优
2. **Boltzmann探索**：按Q值 softmax 概率选择
3. **UCB**：上置信界，平衡探索与利用

### 金融应用
- **离散动作交易**：买入/卖出/持仓三动作决策
- **限价单管理**：最优挂单位置学习
- **配对交易**：价差区间内的交易时机
- **止损止盈**：动态止损位设置

### 深度Q网络（DQN）
- 使用神经网络近似Q函数
- 经验回放：存储历史经验随机采样
- 目标网络：稳定训练目标
- 解决传统Q表的高维状态问题

### 注意事项
- 状态空间设计影响收敛速度
- 奖励稀疏时需设计辅助奖励
- 金融环境非马尔可夫性
- 需要大量样本训练
- 超参数调优困难

---

## 166. 深度强化学习

### 概念定义
深度强化学习结合深度学习与强化学习，使用深度神经网络处理高维状态空间和复杂决策问题。

### 主流算法

#### 1. DQN系列
- **DQN**：首个成功的深度强化学习算法
- **Double DQN**：解决Q值过估计问题
- **Dueling DQN**：分离状态价值和动作优势
- **Rainbow DQN**：集成六种改进方法

#### 2. 策略梯度方法
- **REINFORCE**：蒙特卡洛策略梯度
- **A2C/A3C**：优势Actor-Critic
- **PPO**：近端策略优化，训练稳定
- **TRPO**：信赖域策略优化

#### 3. 连续控制
- **DDPG**：深度确定性策略梯度
- **TD3**：双延迟DDPG
- **SAC**：软 Actor-Critic

### 金融应用
- **连续交易决策**：连续调整仓位比例
- **多因子组合优化**：高维因子空间的资产配置
- **复杂衍生品定价**：高维期权定价
- **市场微观结构**：订单簿级别的交易

### 挑战与限制
- **样本效率**：需要海量交互数据
- **稳定性**：训练过程不稳定
- **可复现性**：随机种子影响结果
- **安全性**：探索可能带来损失

### 注意事项
- 模拟环境应尽可能接近真实市场
- 先在小资金上验证策略
- 考虑市场冲击和滑点
- 模型可能过度拟合历史模式
- 实时监控模型表现，设置熔断机制

---

*文档生成时间：2026-02-25*
*进度：166/200+ 已完成*
