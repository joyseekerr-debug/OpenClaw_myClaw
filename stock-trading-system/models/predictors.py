"""
Phase 2: å¤šæ—¶é—´å°ºåº¦é¢„æµ‹æ¨¡å‹
åŒ…å«LSTMã€XGBoostã€Transformerä¸‰ç§æ¨¡å‹
æ”¯æŒ1åˆ†é’Ÿ/5åˆ†é’Ÿ/15åˆ†é’Ÿ/æ—¥çº¿/å‘¨çº¿å¤šæ—¶é—´å°ºåº¦
"""

import numpy as np
import pandas as pd
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Tuple
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class BaseModel(ABC):
    """é¢„æµ‹æ¨¡å‹åŸºç±»"""
    
    def __init__(self, name: str, config: dict):
        self.name = name
        self.config = config
        self.model = None
        self.is_trained = False
        self.feature_names = []
    
    @abstractmethod
    def train(self, X_train: np.ndarray, y_train: np.ndarray, 
              X_val: Optional[np.ndarray] = None, 
              y_val: Optional[np.ndarray] = None) -> dict:
        """è®­ç»ƒæ¨¡å‹"""
        pass
    
    @abstractmethod
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹"""
        pass
    
    @abstractmethod
    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        pass
    
    @abstractmethod
    def load(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        pass
    
    def get_feature_importance(self) -> Dict[str, float]:
        """è·å–ç‰¹å¾é‡è¦æ€§ï¼ˆå¯é€‰å®ç°ï¼‰"""
        return {}


class LSTMModel(BaseModel):
    """
    LSTMæ·±åº¦å­¦ä¹ æ¨¡å‹
    é€‚ç”¨äº: æ•æ‰æ—¶é—´åºåˆ—é•¿æœŸä¾èµ–å…³ç³»
    æ—¶é—´å°ºåº¦: 1åˆ†é’Ÿ/5åˆ†é’Ÿ/15åˆ†é’Ÿé«˜é¢‘æ•°æ®
    """
    
    def __init__(self, config: dict = None):
        default_config = {
            'sequence_length': 60,
            'hidden_units': 128,
            'num_layers': 2,
            'dropout': 0.2,
            'learning_rate': 0.001,
            'batch_size': 32,
            'epochs': 100,
            'early_stopping_patience': 10
        }
        default_config.update(config or {})
        super().__init__('LSTM', default_config)
        
        self.sequence_length = self.config['sequence_length']
        self.hidden_units = self.config['hidden_units']
        self.scaler = None
    
    def build_model(self, input_shape: Tuple[int, int]):
        """æ„å»ºLSTMæ¨¡å‹"""
        try:
            import torch
            import torch.nn as nn
            
            class LSTMNetwork(nn.Module):
                def __init__(self, input_size, hidden_size, num_layers, dropout, output_size=1):
                    super().__init__()
                    self.hidden_size = hidden_size
                    self.num_layers = num_layers
                    
                    self.lstm = nn.LSTM(
                        input_size=input_size,
                        hidden_size=hidden_size,
                        num_layers=num_layers,
                        dropout=dropout if num_layers > 1 else 0,
                        batch_first=True
                    )
                    
                    self.dropout = nn.Dropout(dropout)
                    self.fc1 = nn.Linear(hidden_size, hidden_size // 2)
                    self.fc2 = nn.Linear(hidden_size // 2, output_size)
                    self.relu = nn.ReLU()
                
                def forward(self, x):
                    lstm_out, _ = self.lstm(x)
                    out = self.dropout(lstm_out[:, -1, :])  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
                    out = self.fc1(out)
                    out = self.relu(out)
                    out = self.fc2(out)
                    return out
            
            self.model = LSTMNetwork(
                input_size=input_shape[1],
                hidden_size=self.hidden_units,
                num_layers=self.config['num_layers'],
                dropout=self.config['dropout']
            )
            
            self.criterion = nn.MSELoss()
            self.optimizer = torch.optim.Adam(
                self.model.parameters(), 
                lr=self.config['learning_rate']
            )
            
            logger.info(f"âœ… LSTMæ¨¡å‹æ„å»ºå®Œæˆ: {input_shape} -> hidden={self.hidden_units}")
            
        except ImportError:
            logger.error("âŒ PyTorchæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨LSTMæ¨¡å‹")
            raise
    
    def train(self, X_train: np.ndarray, y_train: np.ndarray,
              X_val: Optional[np.ndarray] = None,
              y_val: Optional[np.ndarray] = None) -> dict:
        """è®­ç»ƒLSTMæ¨¡å‹"""
        import torch
        from sklearn.preprocessing import StandardScaler
        
        # æ•°æ®å½’ä¸€åŒ–
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)
        if X_val is not None:
            X_val_scaled = self.scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)
        
        # æ„å»ºæ¨¡å‹
        if self.model is None:
            self.build_model((X_train.shape[1], X_train.shape[2]))
        
        # è½¬æ¢ä¸ºTensor
        X_train_tensor = torch.FloatTensor(X_train_scaled)
        y_train_tensor = torch.FloatTensor(y_train).view(-1, 1)
        
        train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = torch.utils.data.DataLoader(
            train_dataset, 
            batch_size=self.config['batch_size'], 
            shuffle=True
        )
        
        # è®­ç»ƒå¾ªç¯
        best_val_loss = float('inf')
        patience_counter = 0
        history = {'train_loss': [], 'val_loss': []}
        
        for epoch in range(self.config['epochs']):
            self.model.train()
            train_loss = 0
            
            for batch_X, batch_y in train_loader:
                self.optimizer.zero_grad()
                outputs = self.model(batch_X)
                loss = self.criterion(outputs, batch_y)
                loss.backward()
                self.optimizer.step()
                train_loss += loss.item()
            
            avg_train_loss = train_loss / len(train_loader)
            history['train_loss'].append(avg_train_loss)
            
            # éªŒè¯
            if X_val is not None:
                self.model.eval()
                with torch.no_grad():
                    X_val_tensor = torch.FloatTensor(X_val_scaled)
                    y_val_tensor = torch.FloatTensor(y_val).view(-1, 1)
                    val_outputs = self.model(X_val_tensor)
                    val_loss = self.criterion(val_outputs, y_val_tensor).item()
                    history['val_loss'].append(val_loss)
                    
                    # Early stopping
                    if val_loss < best_val_loss:
                        best_val_loss = val_loss
                        patience_counter = 0
                    else:
                        patience_counter += 1
                        
                    if patience_counter >= self.config['early_stopping_patience']:
                        logger.info(f"â¹ï¸ Early stopping at epoch {epoch}")
                        break
            
            if (epoch + 1) % 10 == 0:
                logger.info(f"Epoch {epoch+1}/{self.config['epochs']}, "
                          f"Train Loss: {avg_train_loss:.6f}")
        
        self.is_trained = True
        return history
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹"""
        if not self.is_trained:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ")
        
        import torch
        
        self.model.eval()
        X_scaled = self.scaler.transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)
        X_tensor = torch.FloatTensor(X_scaled)
        
        with torch.no_grad():
            predictions = self.model(X_tensor).numpy()
        
        return predictions.flatten()
    
    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        import torch
        import pickle
        
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'config': self.config,
            'scaler': self.scaler
        }, path)
        logger.info(f"ğŸ’¾ LSTMæ¨¡å‹å·²ä¿å­˜: {path}")
    
    def load(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        import torch
        
        checkpoint = torch.load(path)
        self.config = checkpoint['config']
        self.scaler = checkpoint['scaler']
        
        # é‡å»ºæ¨¡å‹
        self.build_model((self.sequence_length, self.config.get('input_size', 30)))
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        self.is_trained = True
        logger.info(f"ğŸ“‚ LSTMæ¨¡å‹å·²åŠ è½½: {path}")


class XGBoostModel(BaseModel):
    """
    XGBoostæœºå™¨å­¦ä¹ æ¨¡å‹
    é€‚ç”¨äº: ä¸­é¢‘æ•°æ®ï¼Œç‰¹å¾é‡è¦æ€§åˆ†æ
    æ—¶é—´å°ºåº¦: 15åˆ†é’Ÿ/60åˆ†é’Ÿ/æ—¥çº¿
    """
    
    def __init__(self, config: dict = None):
        default_config = {
            'n_estimators': 1000,
            'max_depth': 8,
            'learning_rate': 0.01,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'early_stopping_rounds': 50,
            'eval_metric': 'rmse'
        }
        default_config.update(config or {})
        super().__init__('XGBoost', default_config)
    
    def build_model(self):
        """æ„å»ºXGBoostæ¨¡å‹"""
        try:
            import xgboost as xgb
            
            self.model = xgb.XGBRegressor(
                n_estimators=self.config['n_estimators'],
                max_depth=self.config['max_depth'],
                learning_rate=self.config['learning_rate'],
                subsample=self.config['subsample'],
                colsample_bytree=self.config['colsample_bytree'],
                eval_metric=self.config['eval_metric'],
                random_state=42,
                n_jobs=-1
            )
            
            logger.info(f"âœ… XGBoostæ¨¡å‹æ„å»ºå®Œæˆ: {self.config['n_estimators']} estimators")
            
        except ImportError:
            logger.error("âŒ XGBoostæœªå®‰è£…")
            raise
    
    def train(self, X_train: np.ndarray, y_train: np.ndarray,
              X_val: Optional[np.ndarray] = None,
              y_val: Optional[np.ndarray] = None) -> dict:
        """è®­ç»ƒXGBoostæ¨¡å‹"""
        if self.model is None:
            self.build_model()
        
        eval_set = [(X_train, y_train)]
        if X_val is not None and y_val is not None:
            eval_set.append((X_val, y_val))
        
        self.model.fit(
            X_train, y_train,
            eval_set=eval_set,
            early_stopping_rounds=self.config['early_stopping_rounds'],
            verbose=False
        )
        
        self.is_trained = True
        
        # è·å–è®­ç»ƒå†å²
        results = self.model.evals_result()
        
        logger.info(f"âœ… XGBoostè®­ç»ƒå®Œæˆ: best_iteration={self.model.best_iteration}")
        
        return results
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹"""
        if not self.is_trained:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ")
        
        return self.model.predict(X)
    
    def get_feature_importance(self) -> Dict[str, float]:
        """è·å–ç‰¹å¾é‡è¦æ€§"""
        if not self.is_trained:
            return {}
        
        importance = self.model.feature_importances_
        return {f"feature_{i}": imp for i, imp in enumerate(importance)}
    
    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        self.model.save_model(path)
        logger.info(f"ğŸ’¾ XGBoostæ¨¡å‹å·²ä¿å­˜: {path}")
    
    def load(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        import xgboost as xgb
        self.model = xgb.XGBRegressor()
        self.model.load_model(path)
        self.is_trained = True
        logger.info(f"ğŸ“‚ XGBoostæ¨¡å‹å·²åŠ è½½: {path}")


class TransformerModel(BaseModel):
    """
    Transformeræ¨¡å‹
    é€‚ç”¨äº: é•¿åºåˆ—ä¾èµ–æ•æ‰
    æ—¶é—´å°ºåº¦: æ—¥çº¿/å‘¨çº¿é•¿å‘¨æœŸæ•°æ®
    """
    
    def __init__(self, config: dict = None):
        default_config = {
            'd_model': 64,
            'nhead': 4,
            'num_layers': 3,
            'dim_feedforward': 256,
            'dropout': 0.1,
            'learning_rate': 0.0001,
            'batch_size': 32,
            'epochs': 100
        }
        default_config.update(config or {})
        super().__init__('Transformer', default_config)
    
    def build_model(self, input_size: int):
        """æ„å»ºTransformeræ¨¡å‹"""
        try:
            import torch
            import torch.nn as nn
            
            class TransformerNetwork(nn.Module):
                def __init__(self, input_size, d_model, nhead, num_layers, 
                            dim_feedforward, dropout, output_size=1):
                    super().__init__()
                    
                    self.input_projection = nn.Linear(input_size, d_model)
                    
                    encoder_layer = nn.TransformerEncoderLayer(
                        d_model=d_model,
                        nhead=nhead,
                        dim_feedforward=dim_feedforward,
                        dropout=dropout,
                        batch_first=True
                    )
                    self.transformer_encoder = nn.TransformerEncoder(
                        encoder_layer, 
                        num_layers=num_layers
                    )
                    
                    self.decoder = nn.Sequential(
                        nn.Linear(d_model, d_model // 2),
                        nn.ReLU(),
                        nn.Dropout(dropout),
                        nn.Linear(d_model // 2, output_size)
                    )
                
                def forward(self, x):
                    # x: (batch, seq_len, features)
                    x = self.input_projection(x)
                    x = self.transformer_encoder(x)
                    x = x[:, -1, :]  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
                    x = self.decoder(x)
                    return x
            
            self.model = TransformerNetwork(
                input_size=input_size,
                d_model=self.config['d_model'],
                nhead=self.config['nhead'],
                num_layers=self.config['num_layers'],
                dim_feedforward=self.config['dim_feedforward'],
                dropout=self.config['dropout']
            )
            
            self.criterion = nn.MSELoss()
            self.optimizer = torch.optim.Adam(
                self.model.parameters(),
                lr=self.config['learning_rate']
            )
            
            logger.info(f"âœ… Transformeræ¨¡å‹æ„å»ºå®Œæˆ: d_model={self.config['d_model']}")
            
        except ImportError:
            logger.error("âŒ PyTorchæœªå®‰è£…")
            raise
    
    def train(self, X_train: np.ndarray, y_train: np.ndarray,
              X_val: Optional[np.ndarray] = None,
              y_val: Optional[np.ndarray] = None) -> dict:
        """è®­ç»ƒTransformeræ¨¡å‹"""
        import torch
        
        if self.model is None:
            self.build_model(X_train.shape[-1])
        
        # è½¬æ¢ä¸ºTensor
        X_train_tensor = torch.FloatTensor(X_train)
        y_train_tensor = torch.FloatTensor(y_train).view(-1, 1)
        
        train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=self.config['batch_size'],
            shuffle=True
        )
        
        history = {'train_loss': []}
        
        for epoch in range(self.config['epochs']):
            self.model.train()
            train_loss = 0
            
            for batch_X, batch_y in train_loader:
                self.optimizer.zero_grad()
                outputs = self.model(batch_X)
                loss = self.criterion(outputs, batch_y)
                loss.backward()
                self.optimizer.step()
                train_loss += loss.item()
            
            avg_loss = train_loss / len(train_loader)
            history['train_loss'].append(avg_loss)
            
            if (epoch + 1) % 10 == 0:
                logger.info(f"Epoch {epoch+1}/{self.config['epochs']}, Loss: {avg_loss:.6f}")
        
        self.is_trained = True
        return history
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹"""
        if not self.is_trained:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ")
        
        import torch
        
        self.model.eval()
        X_tensor = torch.FloatTensor(X)
        
        with torch.no_grad():
            predictions = self.model(X_tensor).numpy()
        
        return predictions.flatten()
    
    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        import torch
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'config': self.config
        }, path)
        logger.info(f"ğŸ’¾ Transformeræ¨¡å‹å·²ä¿å­˜: {path}")
    
    def load(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        import torch
        checkpoint = torch.load(path)
        self.config = checkpoint['config']
        # é‡å»ºå¹¶åŠ è½½
        self.build_model(self.config.get('input_size', 30))
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.is_trained = True
        logger.info(f"ğŸ“‚ Transformeræ¨¡å‹å·²åŠ è½½: {path}")


class MultiTimeframeEnsemble:
    """
    å¤šæ—¶é—´å°ºåº¦é›†æˆæ¨¡å‹
    æ•´åˆä¸åŒæ—¶é—´å°ºåº¦çš„é¢„æµ‹ç»“æœ
    """
    
    def __init__(self):
        self.models = {}  # å­˜å‚¨ä¸åŒæ—¶é—´å°ºåº¦çš„æ¨¡å‹
        self.weights = {}  # å„æ¨¡å‹æƒé‡
        self.timeframes = ['1min', '5min', '15min', '1h', '1d', '1w']
    
    def add_model(self, timeframe: str, model: BaseModel, weight: float = 1.0):
        """æ·»åŠ æ—¶é—´å°ºåº¦æ¨¡å‹"""
        self.models[timeframe] = model
        self.weights[timeframe] = weight
        logger.info(f"âœ… æ·»åŠ æ¨¡å‹: {timeframe} ({model.name}), weight={weight}")
    
    def predict(self, timeframe_data: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """
        å¤šæ—¶é—´å°ºåº¦é¢„æµ‹
        
        Args:
            timeframe_data: å„æ—¶é—´å°ºåº¦çš„ç‰¹å¾æ•°æ® {timeframe: X}
        
        Returns:
            å„æ—¶é—´å°ºåº¦çš„é¢„æµ‹ç»“æœ
        """
        predictions = {}
        
        for timeframe, X in timeframe_data.items():
            if timeframe in self.models:
                model = self.models[timeframe]
                pred = model.predict(X)
                predictions[timeframe] = pred
        
        return predictions
    
    def ensemble_predict(self, timeframe_data: Dict[str, np.ndarray],
                        method: str = 'weighted_average') -> np.ndarray:
        """
        é›†æˆé¢„æµ‹
        
        Args:
            timeframe_data: å„æ—¶é—´å°ºåº¦çš„ç‰¹å¾æ•°æ®
            method: 'weighted_average', 'voting', 'stacking'
        
        Returns:
            é›†æˆåçš„é¢„æµ‹ç»“æœ
        """
        predictions = self.predict(timeframe_data)
        
        if not predictions:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„é¢„æµ‹ç»“æœ")
        
        if method == 'weighted_average':
            # åŠ æƒå¹³å‡
            weighted_sum = None
            total_weight = 0
            
            for timeframe, pred in predictions.items():
                weight = self.weights.get(timeframe, 1.0)
                if weighted_sum is None:
                    weighted_sum = pred * weight
                else:
                    weighted_sum += pred * weight
                total_weight += weight
            
            return weighted_sum / total_weight
        
        elif method == 'voting':
            # æŠ•ç¥¨æ³•ï¼ˆåˆ†ç±»é—®é¢˜ï¼‰
            # è¿™é‡Œç®€åŒ–ä¸ºå¹³å‡
            return np.mean(list(predictions.values()), axis=0)
        
        else:
            raise ValueError(f"æœªçŸ¥çš„é›†æˆæ–¹æ³•: {method}")
    
    def optimize_weights(self, timeframe_data: Dict[str, np.ndarray],
                        y_true: np.ndarray,
                        metric: str = 'mse'):
        """ä¼˜åŒ–å„æ—¶é—´å°ºåº¦æ¨¡å‹çš„æƒé‡"""
        from scipy.optimize import minimize
        
        predictions = self.predict(timeframe_data)
        timeframes = list(predictions.keys())
        
        def objective(weights):
            # åŠ æƒé¢„æµ‹
            weighted_pred = np.zeros_like(y_true, dtype=float)
            for i, tf in enumerate(timeframes):
                weighted_pred += weights[i] * predictions[tf]
            
            # è®¡ç®—æŸå¤±
            if metric == 'mse':
                return np.mean((y_true - weighted_pred) ** 2)
            elif metric == 'mae':
                return np.mean(np.abs(y_true - weighted_pred))
            else:
                return np.mean((y_true - weighted_pred) ** 2)
        
        # çº¦æŸï¼šæƒé‡å’Œä¸º1ï¼Œéè´Ÿ
        n = len(timeframes)
        constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}
        bounds = [(0, 1) for _ in range(n)]
        initial_weights = [1/n] * n
        
        result = minimize(objective, initial_weights, method='SLSQP',
                         bounds=bounds, constraints=constraints)
        
        # æ›´æ–°æƒé‡
        for i, tf in enumerate(timeframes):
            self.weights[tf] = result.x[i]
        
        logger.info(f"âœ… æƒé‡ä¼˜åŒ–å®Œæˆ: {self.weights}")
        return self.weights


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("="*70)
    print("å¤šæ—¶é—´å°ºåº¦é¢„æµ‹æ¨¡å‹ - Phase 2 æ¨¡å—")
    print("="*70)
    
    # ç¤ºä¾‹ï¼šåˆ›å»ºå„æ—¶é—´å°ºåº¦çš„æ¨¡å‹
    models = {
        '1min': LSTMModel({'sequence_length': 60, 'hidden_units': 64}),
        '5min': LSTMModel({'sequence_length': 48, 'hidden_units': 128}),
        '15min': XGBoostModel({'n_estimators': 500, 'max_depth': 6}),
        '1h': XGBoostModel({'n_estimators': 800, 'max_depth': 8}),
        '1d': TransformerModel({'d_model': 64, 'nhead': 4, 'num_layers': 3}),
        '1w': TransformerModel({'d_model': 32, 'nhead': 2, 'num_layers': 2})
    }
    
    print("\nğŸ“Š æ¨¡å‹é…ç½®:")
    for timeframe, model in models.items():
        print(f"   {timeframe}: {model.name}")
    
    # åˆ›å»ºé›†æˆæ¨¡å‹
    ensemble = MultiTimeframeEnsemble()
    for timeframe, model in models.items():
        ensemble.add_model(timeframe, model, weight=1.0)
    
    print("\nâœ… Phase 2 æ¨¡å‹æ¨¡å—åˆå§‹åŒ–å®Œæˆ")
    print("="*70)
